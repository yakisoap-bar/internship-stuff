{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0d10e79-e142-48d9-9ba8-6fe016c543fd",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "Libraries needed for importing, processing and visualising the datasets are imported here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32336858-a5ee-4785-a348-ba29150bbcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.6.0\n",
      "pandas version: 1.3.2\n",
      "numpy version: 1.20.3\n",
      "seaborn version: 0.11.2\n",
      "GPU absent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-17 12:51:35.526834: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2021-11-17 12:51:35.526853: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: Yaks-Ubuntu\n",
      "2021-11-17 12:51:35.526857: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: Yaks-Ubuntu\n",
      "2021-11-17 12:51:35.526905: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.82.0\n",
      "2021-11-17 12:51:35.526918: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.82.0\n",
      "2021-11-17 12:51:35.526921: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 470.82.0\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import pathlib\n",
    "\n",
    "print(f'tensorflow version: {tf.__version__}')\n",
    "print(f'pandas version: {pd.__version__}')\n",
    "print(f'numpy version: {np.__version__}')\n",
    "print(f'seaborn version: {sns.__version__}')\n",
    "\n",
    "# check tensorflow GPU device support\n",
    "if len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "    print('GPU present')\n",
    "else:\n",
    "    print('GPU absent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3688e15a-2f2c-4803-8abe-92c97be6906c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import Datasets\n",
    "Datasets in the designated folders are imported as `pandas.DataFrame` objects.<br>\n",
    "Dataset signal types and collection locations are automatically determined by their file names:\n",
    "1. full path is read using `pathlib` libaray\n",
    "2. using the naming format `<signal>_<location>_<increment>.csv`, the signal type and collection locations are extracted programmatically\n",
    "3. all signals present are then encoded into a tag, with the signals sorted by alphanumeric order, in the format `{<signal> : <tag>}`, where `<tag>` is `0` to `n` number of signal types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "560947d2-76b2-4d94-af1b-9346b1def48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to load datasets from\n",
    "train_store_path = '../datasets/sa/training'\n",
    "val_store_path = '../datasets/sa/validation'\n",
    "\n",
    "# convert to pathlib Path objects\n",
    "train_dir = pathlib.Path(train_store_path)\n",
    "val_dir = pathlib.Path(val_store_path)\n",
    "\n",
    "# get list of datasets in dir\n",
    "train_ds_paths = sorted(list(train_dir.glob('*.csv')))\n",
    "val_ds_paths = sorted(list(val_dir.glob('*.csv')))\n",
    "\n",
    "# extract classification target from file names\n",
    "train_ds_type = np.array([x.parts[-1].split('_')[:2] for x in train_ds_paths])\n",
    "val_ds_type = np.array([x.parts[-1].split('_')[:2] for x in val_ds_paths])\n",
    "\n",
    "train_ds_order = [s.upper() for s in train_ds_type[:,0]]\n",
    "val_ds_order = [s.upper() for s in val_ds_type[:,0]]\n",
    "\n",
    "train_ds_loc = [s.upper() for s in train_ds_type[:, 1]]\n",
    "val_ds_loc = [s.upper() for s in val_ds_type[:, 1]]\n",
    "\n",
    "# generate signal type tags\n",
    "signal_tags = {k : i for i, k in enumerate(np.unique(sorted([s.upper() for s in train_ds_order])))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fd791c-dbc7-4501-b0b6-c6ad1d673afd",
   "metadata": {},
   "source": [
    "Dataset importing process:\n",
    "1. datasets are loaded using the `pandas.read_csv()` function into 2 different `DataFrame` objects, one containing dataset information and the other containing the data\n",
    "    1. all necessary dataset information like `NumberSamples`, `NumberRecords`, and `SamplingFrequency` are present in the first 10 rows, as such the information object only reads the first 10 rows to save time\n",
    "    2. the first 10 rows are subsequently skipped when importing the data\n",
    "2. unnecessary information are dropped from both dataframes\n",
    "    1. the dataset is separated into invididual records with these in between in the csv: `['TimestampOffset', 'TriggerPosition', 'FastFrameID', 'IDInFastFrame', 'TotalInFastFrame']`. These are dropped to clean up the dataset.\n",
    "    2. the information dataframe contains `['Version', 'DateTime', 'TimestampOffset', 'TriggerPosition', 'FastFrameID', 'IDInFastFrame', 'TotalInFastFrame']`, which are unnecessary and are thus dropped.\n",
    "3. All the processed dataframes are appended into corresponding `specs` and `datasets` lists, split by training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dec36bc-9e92-4393-ab18-1dda212f6de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ../datasets/sa/training/bt_library_1.csv... loaded\n",
      "loading ../datasets/sa/training/bt_office_1.csv... loaded\n",
      "loading ../datasets/sa/training/bt_room_1.csv... loaded\n",
      "loading ../datasets/sa/training/fm_roof_1.csv... loaded\n",
      "loading ../datasets/sa/training/fnet_library_1.csv... loaded\n",
      "loading ../datasets/sa/training/fnet_roof_1.csv... loaded\n",
      "loading ../datasets/sa/training/fnet_room_1.csv... loaded\n",
      "loading ../datasets/sa/training/lte_library_1.csv... loaded\n",
      "loading ../datasets/sa/training/lte_library_2.csv... loaded\n",
      "loading ../datasets/sa/training/lte_library_3.csv... loaded\n",
      "loading ../datasets/sa/training/lte_office_1.csv... loaded\n",
      "loading ../datasets/sa/training/lte_office_2.csv... loaded\n",
      "loading ../datasets/sa/training/lte_room_1.csv... "
     ]
    }
   ],
   "source": [
    "# load the dataset(s)\n",
    "\n",
    "# load dataset information\n",
    "specs = []\n",
    "datasets = []\n",
    "\n",
    "for dataset_paths in [train_ds_paths, val_ds_paths]:\n",
    "    temp_ds = []\n",
    "    temp_specs = []\n",
    "    \n",
    "    for path in dataset_paths:\n",
    "        print(f'loading {path}...', end=' ')\n",
    "\n",
    "        # load dataset details\n",
    "        df_spec = pd.read_csv(path, nrows=10, header=None, index_col=0, names=['info'])\n",
    "        df_spec = df_spec.drop(['Version', 'DateTime', 'TimestampOffset', 'TriggerPosition', 'FastFrameID', 'IDInFastFrame', 'TotalInFastFrame'], axis=0).astype('int')\n",
    "\n",
    "        temp_specs.append(df_spec)\n",
    "\n",
    "        # load data, strip unnecessary bits out\n",
    "        df = pd.read_csv(path, skiprows=10, names=['I', 'Q'])\n",
    "        df = df.loc[~df['I'].isin(['TimestampOffset', 'TriggerPosition', 'FastFrameID', 'IDInFastFrame', 'TotalInFastFrame'])]\n",
    "        df['I'] = df['I'].astype('float')\n",
    "\n",
    "        print(f'loaded')\n",
    "\n",
    "        temp_ds.append(df)\n",
    "        \n",
    "    datasets.append(temp_ds)\n",
    "    specs.append(temp_specs)\n",
    "    \n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697a545e-7bf2-467d-b270-4e09ce83415d",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "Pre-processing process:\n",
    "1. desired record length is defined by `rlength`\n",
    "2. iterate through list of dataframes, starting with training datasets\n",
    "3. extract each record based on `NumberSamples` defined by dataset's corresponding information dataframe regardless of `rlength`\n",
    "4. check if `NumberSamples` is longer or shorter than `rlength`\n",
    "    1. if longer, truncate record to `rlength`\n",
    "    2. if shorter, pad record to `rlength` with a random amount of padding on the left and right of the record\n",
    "    3. else, do nothing and use record as-is\n",
    "5. add record to `temp_processed` which is then added to `processed`, to ensure training and validation data are split up correctly\n",
    "    1. all records follow the format `[<signal>, <tag>, <location>, <data>]`\n",
    "6. records are merged into a training dataframes: `df_train` and `df_test` for training and validation data respectively\n",
    "7. shuffle training and validation datasets to ensure training and minimise overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ac370b-e068-4870-a568-843af6c53122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset(s) into records, extract test dataset\n",
    "processed = []\n",
    "\n",
    "# number of test records to extract\n",
    "ntest = 100\n",
    "rlength = 1024\n",
    "\n",
    "for h in range(len(datasets)):\n",
    "    temp_processed = []\n",
    "    \n",
    "    print(f'\\nType\\t\\tLocation\\tTotal Records\\tSamples/Record')\n",
    "    for i in range(len(datasets[h])):\n",
    "        nrecords = specs[h][i].loc['NumberRecords']['info'] if not h else 400\n",
    "        nsamples = specs[h][i].loc['NumberSamples']['info']\n",
    "        ds_length = datasets[h][i].shape[0]\n",
    "        \n",
    "        # make life easier\n",
    "        ds_order = train_ds_order if not h else val_ds_order\n",
    "        ds_loc = train_ds_loc if not h else val_ds_loc\n",
    "\n",
    "        # sanity check\n",
    "        print(f'{ds_order[i]:<13}\\t{ds_loc[i]:<15}\\t{nrecords:<7}\\t\\t{nsamples:<7}')\n",
    "\n",
    "        # loop through dataset to split \n",
    "        for j in range(nrecords):\n",
    "            # extract sample length worth of samples for each record, then transpose for easier access later\n",
    "            record = datasets[h][i].iloc[(nsamples * j):(nsamples * (j+1))].values.T\n",
    "\n",
    "            # pad shorter records with random padding to rlength\n",
    "            if nsamples < rlength:\n",
    "                # deterine pad amount\n",
    "                pad_length = rlength - nsamples\n",
    "                lpad_length = np.random.randint(0, pad_length+1)\n",
    "                rpad_length = pad_length - lpad_length\n",
    "\n",
    "                # generate pad\n",
    "                lpad = np.zeros((2, lpad_length))\n",
    "                rpad = np.zeros((2, rpad_length))\n",
    "\n",
    "                # concatenate pad\n",
    "                record = np.concatenate([lpad, record, rpad], axis=1)\n",
    "\n",
    "            # truncate longer records to rlength\n",
    "            elif nsamples > rlength:\n",
    "                record = record[:,:rlength]\n",
    "\n",
    "            # add processed record to list\n",
    "            temp_processed.append([ds_order[i], signal_tags[ds_order[i]], ds_loc[i], record])\n",
    "            \n",
    "    processed.append(temp_processed)\n",
    "\n",
    "# convert list into dataframes for later use, randomise, extract test records\n",
    "df_train = pd.DataFrame(processed[0], columns=['signal_type', 'tag', 'location', 'record']).sample(frac=1, random_state=42)\n",
    "df_test = pd.DataFrame(processed[1], columns=['signal_type', 'tag', 'location', 'record']).sample(frac=1, random_state=42)\n",
    "\n",
    "# print dataset statistics\n",
    "print(f'\\n{\"Stats\":^30}')\n",
    "print(f'Dataset\\tLength\\tRecords/Sample')\n",
    "print(f'Train\\t{df_train.shape[0]:<5}\\t{df_train[\"record\"].iloc[0].shape[1]}')\n",
    "print(f'Test\\t{df_test.shape[0]:<5}\\t{df_train[\"record\"].iloc[0].shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303cc390-d8f1-43a1-86ae-3e7c01ab49b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some sample signals\n",
    "fig1 = plt.figure(1, figsize=(25, 12))\n",
    "fig2 = plt.figure(2, figsize=(25, 12))\n",
    "\n",
    "for i, key in enumerate(list(signal_tags.keys())):\n",
    "    # training data\n",
    "    ip, qd = df_train[df_train['signal_type'] == key]['record'].sample().iloc[0]\n",
    "    \n",
    "    ax = fig1.add_subplot(5, 1, i+1)\n",
    "\n",
    "    ax.plot(ip)\n",
    "    ax.plot(qd)\n",
    "    \n",
    "    ax.set_title(f'{key} sample')\n",
    "    \n",
    "    # validation data\n",
    "    ip, qd = df_test[df_test['signal_type'] == key]['record'].sample().iloc[0]\n",
    "    \n",
    "    ax = fig2.add_subplot(5, 1, i+1)\n",
    "\n",
    "    ax.plot(ip)\n",
    "    ax.plot(qd)\n",
    "    \n",
    "    ax.set_title(f'{key} sample')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a718967-2123-419a-8071-39f732cff7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define one hot encode function\n",
    "def one_hot(arr, n_cat):\n",
    "    output = []\n",
    "    for n in arr:\n",
    "        result = np.zeros(n_cat)\n",
    "        result[n] = 1\n",
    "\n",
    "        output.append(result)\n",
    "\n",
    "    return np.array(output, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615eb9cb-0b8d-459f-9a7f-6ae34fc33532",
   "metadata": {},
   "source": [
    "Due to how the model works, the data and its corresponding classification targets need to be processed as so:\n",
    "1. data is reshaped from the format `[2, <record length>]` to `[2, <record length>, 1]`\n",
    "2. classification targets are one-hot encoded using the above defined `one_hot` function\n",
    "    1. it turns a numeric input into an array of mostly zeros with a single 1 in the input's number as the array position, e.g. 4 standing for Wifi into `[0, 0, 0, 0, 1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e744e4c7-8f22-4628-a9e1-7c2ce033acfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract train and test data\n",
    "X_train = np.concatenate(df_train['record'].values).reshape((df_train.shape[0], 2, rlength, 1))\n",
    "y_train = one_hot(df_train['tag'].values, len(signal_tags))\n",
    "\n",
    "X_test = np.concatenate(df_test['record'].values).reshape((df_test.shape[0], 2, rlength, 1))\n",
    "y_test = one_hot(df_test['tag'].values, len(signal_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6db35ed-1c64-49af-8ad4-1dab0df5fd67",
   "metadata": {},
   "source": [
    "## Model construction and training\n",
    "This particular model cannot be fully constructed using the Keras API due to the use of skip-connections, instead the Tensorflow Functional API is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4c0ee3-bf7b-46bb-8a93-8b80de3cf501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model stuff\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, concatenate, Dense, Input, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378b5434-ab38-45a2-b91d-14d5bc155a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for model segments\n",
    "def res_unit(x, dim, n):\n",
    "    '''\n",
    "    function that creates a residual unit for each residual stack.\n",
    "\n",
    "    INPUT PARAMETERS\n",
    "    x: layer to connect to\n",
    "    dim: size of layer\n",
    "    n: number of units to create\n",
    "    '''\n",
    "\n",
    "    for _ in range(n):\n",
    "        u = Conv2D(dim, 2, activation='relu', padding='same')(x)\n",
    "        u = Conv2D(dim, 2, activation='linear', padding='same')(u)\n",
    "\n",
    "        # skip-con\n",
    "        x = concatenate([u, x])\n",
    "\n",
    "    return x\n",
    "\n",
    "def res_stack(x, dim):\n",
    "    '''\n",
    "    function that creates a residual stack for the model\n",
    "\n",
    "    INPUT PARAMETERS\n",
    "    x: layer to connect to\n",
    "    dim: size of stack\n",
    "    '''\n",
    "\n",
    "    s = Conv2D(dim, 1, activation='linear', padding='same')(x)\n",
    "    s = res_unit(s, dim, 2)\n",
    "    s = MaxPooling2D(2, padding='same')(s)\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd38230-e189-46a0-afc9-12c7a00909ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create main model\n",
    "def create_model(in_dim, out_dim):\n",
    "    '''\n",
    "    function to construct the actual resnet model.\n",
    "\n",
    "    INPUT PARAMETERS\n",
    "    in_dim: dimensions of input\n",
    "    out_dim: size of output\n",
    "    '''\n",
    "\n",
    "    input_layer = Input(in_dim)\n",
    "    \n",
    "    # res stacks\n",
    "    x = res_stack(input_layer, 512)\n",
    "    x = res_stack(x, 256)\n",
    "    x = res_stack(x, 128)\n",
    "    x = res_stack(x, 64)\n",
    "    x = res_stack(x, 32)\n",
    "    x = res_stack(x, 16)\n",
    "\n",
    "    # fully connected layers\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "\n",
    "    output_layer = Dense(out_dim, activation='softmax')(x)\n",
    "\n",
    "    # turn layers into model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer, name='resnet_rf_classification_model')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11818d0b-07fc-4b0a-bd24-74736a7f80f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model((2, rlength, 1), len(signal_tags))\n",
    "\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe20337f-2f27-4224-8ae7-3a89fd476384",
   "metadata": {},
   "source": [
    "The model is trained here, with early stopping to minimise overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e319b07f-8f26-4443-85cb-3799a0cf2ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "# import metrics\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "#early stopping\n",
    "callback = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min')\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0005), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, callbacks=[callback], validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2b2b92-6582-4daa-bad0-4aa7e0720953",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48956100-9999-4418-9e43-f7aea982b540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model performance on standard dataset\n",
    "results = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'loss: {results[0]:.3f} | accuracy: {results[1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c96dc8b-6dbc-4228-b567-d66e1cb5ef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot out loss and accuracy graphs\n",
    "train_hist = history.history\n",
    "\n",
    "fig = plt.figure(1, figsize=(14, 7))\n",
    "\n",
    "# loss graph\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.plot(train_hist['loss'], label='training')\n",
    "ax1.plot(train_hist['val_loss'], label='validation')\n",
    "ax1.set_xlabel('epochs')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(b=True)\n",
    "\n",
    "# accuracy graph\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.plot(train_hist['accuracy'], label='training')\n",
    "ax2.plot(train_hist['val_accuracy'], label='validation')\n",
    "ax2.set_xlabel('epochs')\n",
    "ax2.set_ylabel('accuracy')\n",
    "ax2.set_title('Training Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(b=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56f7933-1ffb-49b9-b4e5-36a0fbb49669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrices\n",
    "cm_pred = model.predict(X_test)\n",
    "\n",
    "# process results\n",
    "temp = []\n",
    "for row in cm_pred:\n",
    "    temp.append(np.argmax(row))  \n",
    "cm_pred = np.array(temp)\n",
    "\n",
    "cm_truth = df_test['tag'].values\n",
    "\n",
    "# generate matrix\n",
    "tags = list(signal_tags.keys())\n",
    "df_cm = pd.DataFrame(tf.math.confusion_matrix(cm_truth, cm_pred).numpy(), index=tags, columns=tags)\n",
    "\n",
    "# plot matrix\n",
    "fig = plt.figure(2, figsize=(10, 7))\n",
    "ax = fig.add_axes([1, 1, 1, 1])\n",
    "sns.heatmap(df_cm, annot=True, fmt='d', ax=ax)\n",
    "ax.set_title('Prediction Confusion Matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b00c6296-cacb-4196-b304-b0956d5ab3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-08 20:18:04.231093: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_models/SA_resnet/assets\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "model.save('../saved_models/SA_resnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97dbc05e-e9a8-43cf-889b-cf7b7e240639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-12 16:37:20.718648: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpv4up5xct/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-12 16:37:22.398855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-12 16:37:22.399033: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2021-11-12 16:37:22.399110: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2021-11-12 16:37:22.399305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-12 16:37:22.399476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-12 16:37:22.399630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-12 16:37:22.399821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-12 16:37:22.399980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-12 16:37:22.400115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5114 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2021-11-12 16:37:22.420735: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.009ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "\n",
      "2021-11-12 16:37:22.786361: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.\n",
      "2021-11-12 16:37:22.786386: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.\n",
      "2021-11-12 16:37:22.819245: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "# convert model to tflite\n",
    "import tensorflow as tf\n",
    "\n",
    "# model = tf.keras.models.load_model('./saved_models/SA_resnet')\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# converter.target_spec.supported_ops = [\n",
    "#   tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
    "#   tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
    "# ]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# save model\n",
    "with open('./saved_models/sa_resnet.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "    \n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
